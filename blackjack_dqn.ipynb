{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "blackjack_dqn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mia1996/rlcard-tutoirial/blob/master/blackjack_dqn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miBl4S8JARzX",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "# <a href='https://github.com/datamllab/rlcard'> <center> <img src='https://miro.medium.com/max/1000/1*_9abDpNTM9Cbsd2HEXYm9Q.png' width=500 class='center' /></a> \n",
        "\n",
        "## **Deep-Q Learning on Blackjack**\n",
        "This example is to use Deep-Q learning to train an agent on Blackjack. We aim to use this example to show how reinforcement learning algorithms can be developed and applied in our toolkit. We design a run function which plays one complete game and provides the data for training RL agents. The example is shown below:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZbqww8VXSOx",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "* First, we install RLcard and Tensorflow. To use Tensorflow implementation of the example algorithms, we recommend installing the supported verison of Tensorflow with `rlcard[tensorflow]`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQ8CiXAJjQGi",
        "colab_type": "code",
        "outputId": "ce7e12bc-10c9-49c5-f37a-f578e242d6a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "pip install rlcard"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting rlcard\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/d0/db4f9c26f1ca2d428738c42a2b4de6bf7a891796814ea89ade293937df48/rlcard-0.2.4.tar.gz (6.6MB)\n",
            "\u001b[K     |████████████████████████████████| 6.7MB 5.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.3 in /usr/local/lib/python3.6/dist-packages (from rlcard) (1.18.4)\n",
            "Requirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.6/dist-packages (from rlcard) (3.2.1)\n",
            "Requirement already satisfied: pillow>=5.2.0 in /usr/local/lib/python3.6/dist-packages (from rlcard) (7.0.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from rlcard) (1.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from rlcard) (20.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0->rlcard) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0->rlcard) (1.2.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0->rlcard) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0->rlcard) (2.8.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->rlcard) (1.12.0)\n",
            "Building wheels for collected packages: rlcard\n",
            "  Building wheel for rlcard (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rlcard: filename=rlcard-0.2.4-cp36-none-any.whl size=6746015 sha256=baaa4f1b86f883fb4d5b9d9e0f229d1bd57a6263f1e9e26132d3e0436e1a00b4\n",
            "  Stored in directory: /root/.cache/pip/wheels/c3/a0/b5/bb8f10f5f5b3b8727ee0a135432168c3860ee23558366a4b5a\n",
            "Successfully built rlcard\n",
            "Installing collected packages: rlcard\n",
            "Successfully installed rlcard-0.2.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_A_Br3Jj0xW",
        "colab_type": "code",
        "outputId": "2249565b-dbe9-4aae-f820-543e280a8807",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "pip install rlcard[tensorflow]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: rlcard[tensorflow] in /usr/local/lib/python3.6/dist-packages (0.2.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from rlcard[tensorflow]) (20.4)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from rlcard[tensorflow]) (1.1.0)\n",
            "Requirement already satisfied: pillow>=5.2.0 in /usr/local/lib/python3.6/dist-packages (from rlcard[tensorflow]) (7.0.0)\n",
            "Requirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.6/dist-packages (from rlcard[tensorflow]) (3.2.1)\n",
            "Requirement already satisfied: numpy>=1.16.3 in /usr/local/lib/python3.6/dist-packages (from rlcard[tensorflow]) (1.18.4)\n",
            "Collecting tensorflow<2.0,>=1.14; extra == \"tensorflow\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/36/9a02e27f0ec248b676a380ffe910c1858e3af3027c0d4d513dd0b56a5613/tensorflow-1.15.3-cp36-cp36m-manylinux2010_x86_64.whl (110.5MB)\n",
            "\u001b[K     |████████████████████████████████| 110.5MB 46kB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->rlcard[tensorflow]) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->rlcard[tensorflow]) (1.12.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0->rlcard[tensorflow]) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0->rlcard[tensorflow]) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0->rlcard[tensorflow]) (1.2.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.0,>=1.14; extra == \"tensorflow\"->rlcard[tensorflow]) (0.34.2)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.0,>=1.14; extra == \"tensorflow\"->rlcard[tensorflow]) (3.2.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.0,>=1.14; extra == \"tensorflow\"->rlcard[tensorflow]) (0.9.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.0,>=1.14; extra == \"tensorflow\"->rlcard[tensorflow]) (3.10.0)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 44.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.0,>=1.14; extra == \"tensorflow\"->rlcard[tensorflow]) (1.29.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.0,>=1.14; extra == \"tensorflow\"->rlcard[tensorflow]) (0.8.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.0,>=1.14; extra == \"tensorflow\"->rlcard[tensorflow]) (0.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.0,>=1.14; extra == \"tensorflow\"->rlcard[tensorflow]) (1.12.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.0,>=1.14; extra == \"tensorflow\"->rlcard[tensorflow]) (1.1.2)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 46.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.0,>=1.14; extra == \"tensorflow\"->rlcard[tensorflow]) (1.0.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow<2.0,>=1.14; extra == \"tensorflow\"->rlcard[tensorflow]) (47.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow<2.0,>=1.14; extra == \"tensorflow\"->rlcard[tensorflow]) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow<2.0,>=1.14; extra == \"tensorflow\"->rlcard[tensorflow]) (3.2.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow<2.0,>=1.14; extra == \"tensorflow\"->rlcard[tensorflow]) (2.10.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow<2.0,>=1.14; extra == \"tensorflow\"->rlcard[tensorflow]) (1.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow<2.0,>=1.14; extra == \"tensorflow\"->rlcard[tensorflow]) (3.1.0)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=919a1271129880057ca8c67557868dd703da4e7e91244cf80bc98b835ac32fe4\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: tensorflow-probability 0.10.0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: gast, tensorboard, tensorflow-estimator, tensorflow\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorboard 2.2.2\n",
            "    Uninstalling tensorboard-2.2.2:\n",
            "      Successfully uninstalled tensorboard-2.2.2\n",
            "  Found existing installation: tensorflow-estimator 2.2.0\n",
            "    Uninstalling tensorflow-estimator-2.2.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.2.0\n",
            "  Found existing installation: tensorflow 2.2.0\n",
            "    Uninstalling tensorflow-2.2.0:\n",
            "      Successfully uninstalled tensorflow-2.2.0\n",
            "Successfully installed gast-0.2.2 tensorboard-1.15.0 tensorflow-1.15.3 tensorflow-estimator-1.15.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2ltkfinYmiU",
        "colab_type": "text"
      },
      "source": [
        "* Then, we import the modules that are wraped up in rlcard packages. Before the training process, we make the environment, set the iterations numbers, initialize the memory size and the frenquency that we evaluate the performance. After training the agent, we will save the performance data and learning curves to ./log."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_8Kuf47kghG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "import rlcard\n",
        "from rlcard.agents import DQNAgent\n",
        "from rlcard.utils import set_global_seed, tournament\n",
        "from rlcard.utils import Logger"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bqfMncnJTYU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make environment\n",
        "env = rlcard.make('blackjack', config={'seed': 0})\n",
        "eval_env = rlcard.make('blackjack', config={'seed': 0})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhgGYyick13x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set the iterations numbers and how frequently we evaluate/save plot\n",
        "evaluate_every = 100\n",
        "evaluate_num = 10000\n",
        "episode_num = 5000\n",
        "\n",
        "# The intial memory size\n",
        "memory_init_size = 100\n",
        "\n",
        "# Train the agent every X steps\n",
        "train_every = 1\n",
        "\n",
        "# The paths for saving the logs and learning curves\n",
        "log_dir = './experiments/blackjack_dqn_result/'\n",
        "\n",
        "# Set a global seed\n",
        "tf.compat.v1.set_random_seed(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNR02_JzZJfe",
        "colab_type": "text"
      },
      "source": [
        "* Before starting the training process, we initialize a global step and set up the DQN agents. The Logger is used to plot the learning curve and save it to the same directory as we set up for log.  Now we start to train DQN on Lecuc Hold'em. The training logs and the learning curves are shown as below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYAj8Q22k5e2",
        "colab_type": "code",
        "outputId": "706fbe1e-a2bb-4b88-fc6a-23d5c5b917f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "with tf.Session() as sess:\n",
        "\n",
        "    # Initialize a global step\n",
        "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
        "\n",
        "    # Set up the agents\n",
        "    agent = DQNAgent(sess,\n",
        "                     scope='dqn',\n",
        "                     action_num=env.action_num,\n",
        "                     replay_memory_init_size=memory_init_size,\n",
        "                     train_every=train_every,\n",
        "                     state_shape=env.state_shape,\n",
        "                     mlp_layers=[10,10])\n",
        "    env.set_agents([agent])\n",
        "    eval_env.set_agents([agent])\n",
        "\n",
        "    # Initialize global variables\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    # Init a Logger to plot the learning curve\n",
        "    logger = Logger(log_dir)\n",
        "\n",
        "    for episode in range(episode_num):\n",
        "\n",
        "        # Generate data from the environment\n",
        "        trajectories, _ = env.run(is_training=True)\n",
        "\n",
        "        # Feed transitions into agent memory, and train the agent\n",
        "        for ts in trajectories[0]:\n",
        "            agent.feed(ts)\n",
        "\n",
        "        # Evaluate the performance. Play with random agents.\n",
        "        if episode % evaluate_every == 0:\n",
        "            logger.log_performance(env.timestep, tournament(eval_env, evaluate_num)[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/rlcard/agents/dqn_agent.py:239: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/rlcard/agents/dqn_agent.py:256: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/rlcard/agents/dqn_agent.py:267: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/normalization.py:327: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1634: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/rlcard/agents/dqn_agent.py:280: The name tf.squared_difference is deprecated. Please use tf.math.squared_difference instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/rlcard/agents/dqn_agent.py:242: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/rlcard/agents/dqn_agent.py:242: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/rlcard/agents/dqn_agent.py:242: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/rlcard/agents/dqn_agent.py:244: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/rlcard/agents/dqn_agent.py:247: get_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.get_global_step\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  1\n",
            "  reward       |  -0.7364\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 100, rl-loss: 1.0503010749816895WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/rlcard/agents/dqn_agent.py:364: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n",
            "\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 132, rl-loss: 0.6607481241226196\n",
            "----------------------------------------\n",
            "  timestep     |  132\n",
            "  reward       |  -0.1653\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 264, rl-loss: 0.753758430480957\n",
            "----------------------------------------\n",
            "  timestep     |  264\n",
            "  reward       |  -0.1398\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 392, rl-loss: 0.660798966884613\n",
            "----------------------------------------\n",
            "  timestep     |  392\n",
            "  reward       |  -0.1019\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 529, rl-loss: 0.7499272227287292\n",
            "----------------------------------------\n",
            "  timestep     |  529\n",
            "  reward       |  -0.0887\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 659, rl-loss: 0.7489523887634277\n",
            "----------------------------------------\n",
            "  timestep     |  659\n",
            "  reward       |  -0.0735\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 796, rl-loss: 0.6210151314735413\n",
            "----------------------------------------\n",
            "  timestep     |  796\n",
            "  reward       |  -0.064\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 935, rl-loss: 0.788637638092041\n",
            "----------------------------------------\n",
            "  timestep     |  935\n",
            "  reward       |  -0.0787\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 1074, rl-loss: 0.5471451878547668\n",
            "----------------------------------------\n",
            "  timestep     |  1074\n",
            "  reward       |  -0.0914\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 1100, rl-loss: 0.7084033489227295\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 1232, rl-loss: 0.5008317232131958\n",
            "----------------------------------------\n",
            "  timestep     |  1232\n",
            "  reward       |  -0.0758\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 1364, rl-loss: 0.5806444883346558\n",
            "----------------------------------------\n",
            "  timestep     |  1364\n",
            "  reward       |  -0.0706\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 1489, rl-loss: 0.6233082413673401\n",
            "----------------------------------------\n",
            "  timestep     |  1489\n",
            "  reward       |  -0.0618\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 1626, rl-loss: 0.6641694903373718\n",
            "----------------------------------------\n",
            "  timestep     |  1626\n",
            "  reward       |  -0.0756\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 1770, rl-loss: 0.4438415765762329\n",
            "----------------------------------------\n",
            "  timestep     |  1770\n",
            "  reward       |  -0.0456\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 1905, rl-loss: 0.6584020853042603\n",
            "----------------------------------------\n",
            "  timestep     |  1905\n",
            "  reward       |  -0.0813\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 2040, rl-loss: 0.6156619191169739\n",
            "----------------------------------------\n",
            "  timestep     |  2040\n",
            "  reward       |  -0.075\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 2100, rl-loss: 0.6999431848526001\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 2166, rl-loss: 0.592107355594635\n",
            "----------------------------------------\n",
            "  timestep     |  2166\n",
            "  reward       |  -0.0738\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 2306, rl-loss: 0.5401313304901123\n",
            "----------------------------------------\n",
            "  timestep     |  2306\n",
            "  reward       |  -0.0814\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 2442, rl-loss: 0.6558364629745483\n",
            "----------------------------------------\n",
            "  timestep     |  2442\n",
            "  reward       |  -0.06\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 2588, rl-loss: 0.5347142219543457\n",
            "----------------------------------------\n",
            "  timestep     |  2588\n",
            "  reward       |  -0.0565\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 2734, rl-loss: 0.3285204768180847\n",
            "----------------------------------------\n",
            "  timestep     |  2734\n",
            "  reward       |  -0.072\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 2873, rl-loss: 0.6231152415275574\n",
            "----------------------------------------\n",
            "  timestep     |  2873\n",
            "  reward       |  -0.0514\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 3019, rl-loss: 0.5220484137535095\n",
            "----------------------------------------\n",
            "  timestep     |  3019\n",
            "  reward       |  -0.0582\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 3100, rl-loss: 0.425850510597229\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 3168, rl-loss: 0.6122305393218994\n",
            "----------------------------------------\n",
            "  timestep     |  3168\n",
            "  reward       |  -0.0661\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 3308, rl-loss: 0.5621050596237183\n",
            "----------------------------------------\n",
            "  timestep     |  3308\n",
            "  reward       |  -0.0671\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 3443, rl-loss: 0.6519292593002319\n",
            "----------------------------------------\n",
            "  timestep     |  3443\n",
            "  reward       |  -0.0558\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 3575, rl-loss: 0.540397047996521\n",
            "----------------------------------------\n",
            "  timestep     |  3575\n",
            "  reward       |  -0.0744\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 3727, rl-loss: 0.4402461647987366\n",
            "----------------------------------------\n",
            "  timestep     |  3727\n",
            "  reward       |  -0.069\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 3855, rl-loss: 0.6080322265625\n",
            "----------------------------------------\n",
            "  timestep     |  3855\n",
            "  reward       |  -0.0781\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 3996, rl-loss: 0.4945164620876312\n",
            "----------------------------------------\n",
            "  timestep     |  3996\n",
            "  reward       |  -0.0717\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 4100, rl-loss: 0.5923049449920654\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 4141, rl-loss: 0.7264725565910339\n",
            "----------------------------------------\n",
            "  timestep     |  4141\n",
            "  reward       |  -0.0745\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 4281, rl-loss: 0.5494579076766968\n",
            "----------------------------------------\n",
            "  timestep     |  4281\n",
            "  reward       |  -0.0715\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 4420, rl-loss: 0.6891418099403381\n",
            "----------------------------------------\n",
            "  timestep     |  4420\n",
            "  reward       |  -0.0666\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 4572, rl-loss: 0.5492649078369141\n",
            "----------------------------------------\n",
            "  timestep     |  4572\n",
            "  reward       |  -0.0672\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 4703, rl-loss: 0.5817355513572693\n",
            "----------------------------------------\n",
            "  timestep     |  4703\n",
            "  reward       |  -0.0683\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 4846, rl-loss: 0.40846362709999084\n",
            "----------------------------------------\n",
            "  timestep     |  4846\n",
            "  reward       |  -0.0646\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 4991, rl-loss: 0.4794807434082031\n",
            "----------------------------------------\n",
            "  timestep     |  4991\n",
            "  reward       |  -0.0554\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 5100, rl-loss: 0.5673857927322388\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 5125, rl-loss: 0.5943800210952759\n",
            "----------------------------------------\n",
            "  timestep     |  5125\n",
            "  reward       |  -0.0654\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 5272, rl-loss: 0.7976393699645996\n",
            "----------------------------------------\n",
            "  timestep     |  5272\n",
            "  reward       |  -0.0505\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 5418, rl-loss: 0.5581241846084595\n",
            "----------------------------------------\n",
            "  timestep     |  5418\n",
            "  reward       |  -0.0499\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 5551, rl-loss: 0.6242834329605103\n",
            "----------------------------------------\n",
            "  timestep     |  5551\n",
            "  reward       |  -0.0617\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 5688, rl-loss: 0.5192543268203735\n",
            "----------------------------------------\n",
            "  timestep     |  5688\n",
            "  reward       |  -0.0463\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 5830, rl-loss: 0.4703691005706787\n",
            "----------------------------------------\n",
            "  timestep     |  5830\n",
            "  reward       |  -0.0716\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 5967, rl-loss: 0.4868971109390259\n",
            "----------------------------------------\n",
            "  timestep     |  5967\n",
            "  reward       |  -0.0634\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 6100, rl-loss: 0.5730417966842651\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 6115, rl-loss: 0.6232990026473999\n",
            "----------------------------------------\n",
            "  timestep     |  6115\n",
            "  reward       |  -0.076\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 6256, rl-loss: 0.7766022682189941\n",
            "----------------------------------------\n",
            "  timestep     |  6256\n",
            "  reward       |  -0.0708\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 6398, rl-loss: 0.47600382566452026\n",
            "----------------------------------------\n",
            "  timestep     |  6398\n",
            "  reward       |  -0.0601\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 6535, rl-loss: 0.46180039644241333\n",
            "----------------------------------------\n",
            "  timestep     |  6535\n",
            "  reward       |  -0.0665\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 6668, rl-loss: 0.5147315263748169\n",
            "----------------------------------------\n",
            "  timestep     |  6668\n",
            "  reward       |  -0.0791\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 6808, rl-loss: 0.6380113959312439\n",
            "----------------------------------------\n",
            "  timestep     |  6808\n",
            "  reward       |  -0.0645\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 6944, rl-loss: 0.4691145420074463"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06n2QSTDIqb0",
        "colab_type": "code",
        "outputId": "683006b4-535a-4561-cb25-b0b4499c46c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        }
      },
      "source": [
        "    # Close files in the logger\n",
        "    logger.close_files()\n",
        "\n",
        "    # Plot the learning curve\n",
        "    logger.plot('DQN')\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "./experiments/blackjack_dqn_result/performance.csv\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtJ7xdSIaPw3",
        "colab_type": "text"
      },
      "source": [
        "#### In Blackjack, the player will get a payoff at the end of the game:\n",
        "1 if the player wins, -1 if the player loses, and 0 if it is a tie. The performance is measured by the average payoff the player obtains by playing 10000 episodes. The above example shows that the agent achieves better and better performance during training. The logs and learning curves are saved in ./experiments/blackjack_dqn_result/.\n",
        "\n",
        "### Now you have your trained Blackjack game!"
      ]
    }
  ]
}
