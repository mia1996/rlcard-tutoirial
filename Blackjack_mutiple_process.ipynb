{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Blackjack_mutiple_process.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mia1996/rlcard-tutoirial/blob/master/Blackjack_mutiple_process.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A88uUTIFLlCX",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "# <a href='https://github.com/datamllab/rlcard'> <center> <img src='https://miro.medium.com/max/1000/1*_9abDpNTM9Cbsd2HEXYm9Q.png' width=500 class='center' /></a> \n",
        "\n",
        "## **Running Multiple Processes on Blackjack**\n",
        "The environments can be run with multiple processes to accelerate the training. Below is an example to train DQN on Blackjack with multiple processes.The example is shown below:\n",
        "\n",
        "Note that we must use if `__name__ == '__main__' `for multiprocessing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkCeAJhjdjLr",
        "colab_type": "text"
      },
      "source": [
        "* First, we install Rlcard and Tensorflow. To use Tensorflow implementation of the example algorithms, we recommend to install the supported verison of Tensorflow with rlcard[tensorflow]."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQ8CiXAJjQGi",
        "colab_type": "code",
        "outputId": "14ecc649-375b-4cab-8c7a-a9a6a4f5aaf1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "pip install rlcard"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: rlcard in /usr/local/lib/python3.6/dist-packages (0.2.4)\n",
            "Requirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.6/dist-packages (from rlcard) (3.2.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from rlcard) (20.4)\n",
            "Requirement already satisfied: pillow>=5.2.0 in /usr/local/lib/python3.6/dist-packages (from rlcard) (7.0.0)\n",
            "Requirement already satisfied: numpy>=1.16.3 in /usr/local/lib/python3.6/dist-packages (from rlcard) (1.18.4)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from rlcard) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0->rlcard) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0->rlcard) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0->rlcard) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0->rlcard) (0.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->rlcard) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_A_Br3Jj0xW",
        "colab_type": "code",
        "outputId": "2f70bb5d-3a5a-4ee6-d02b-7b44af68f919",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        }
      },
      "source": [
        "pip install rlcard[tensorflow]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: rlcard[tensorflow] in /usr/local/lib/python3.6/dist-packages (0.2.4)\n",
            "Requirement already satisfied: pillow>=5.2.0 in /usr/local/lib/python3.6/dist-packages (from rlcard[tensorflow]) (7.0.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from rlcard[tensorflow]) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.16.3 in /usr/local/lib/python3.6/dist-packages (from rlcard[tensorflow]) (1.18.4)\n",
            "Requirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.6/dist-packages (from rlcard[tensorflow]) (3.2.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from rlcard[tensorflow]) (20.4)\n",
            "Requirement already satisfied: tensorflow<2.0,>=1.14; extra == \"tensorflow\" in /usr/local/lib/python3.6/dist-packages (from rlcard[tensorflow]) (1.15.3)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0->rlcard[tensorflow]) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0->rlcard[tensorflow]) (1.2.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0->rlcard[tensorflow]) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0->rlcard[tensorflow]) (0.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->rlcard[tensorflow]) (1.12.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.0,>=1.14; extra == \"tensorflow\"->rlcard[tensorflow]) (3.2.1)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.0,>=1.14; extra == \"tensorflow\"->rlcard[tensorflow]) (0.34.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.0,>=1.14; extra == \"tensorflow\"->rlcard[tensorflow]) (0.9.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.0,>=1.14; extra == \"tensorflow\"->rlcard[tensorflow]) (1.29.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.0,>=1.14; extra == \"tensorflow\"->rlcard[tensorflow]) (1.0.8)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.0,>=1.14; extra == \"tensorflow\"->rlcard[tensorflow]) (1.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.0,>=1.14; extra == \"tensorflow\"->rlcard[tensorflow]) (1.12.1)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.0,>=1.14; extra == \"tensorflow\"->rlcard[tensorflow]) (1.15.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.0,>=1.14; extra == \"tensorflow\"->rlcard[tensorflow]) (3.10.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.0,>=1.14; extra == \"tensorflow\"->rlcard[tensorflow]) (0.8.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.0,>=1.14; extra == \"tensorflow\"->rlcard[tensorflow]) (0.2.0)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.0,>=1.14; extra == \"tensorflow\"->rlcard[tensorflow]) (0.2.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.0,>=1.14; extra == \"tensorflow\"->rlcard[tensorflow]) (1.1.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow<2.0,>=1.14; extra == \"tensorflow\"->rlcard[tensorflow]) (2.10.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow<2.0,>=1.14; extra == \"tensorflow\"->rlcard[tensorflow]) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow<2.0,>=1.14; extra == \"tensorflow\"->rlcard[tensorflow]) (3.2.2)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow<2.0,>=1.14; extra == \"tensorflow\"->rlcard[tensorflow]) (47.1.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow<2.0,>=1.14; extra == \"tensorflow\"->rlcard[tensorflow]) (1.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow<2.0,>=1.14; extra == \"tensorflow\"->rlcard[tensorflow]) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuMuqdmDdmQV",
        "colab_type": "text"
      },
      "source": [
        "* Then, we import the modules that are wraped up in rlcard packages. Then, we set the iterations numbers, the initial memory size and how frequently we evaluate the performance. Finally, we save the performance data and learning curves to our current path as ./log. After training the game, we could see the training performance and plot as below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6f7kJGML8Dk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "import rlcard\n",
        "from rlcard.agents import DQNAgent\n",
        "from rlcard.utils import set_global_seed, tournament\n",
        "from rlcard.utils import Logger"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDdHoimFMWUy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main():\n",
        "    # Make environment\n",
        "    env = rlcard.make('blackjack', config={'seed': 0, 'env_num': 4})\n",
        "    eval_env = rlcard.make('blackjack', config={'seed': 0, 'env_num': 4})\n",
        "\n",
        "    # Set the iterations numbers and how frequently we evaluate performance\n",
        "    evaluate_every = 100\n",
        "    evaluate_num = 10000\n",
        "    iteration_num = 5000\n",
        "\n",
        "    # The intial memory size\n",
        "    memory_init_size = 100\n",
        "\n",
        "    # Train the agent every X steps\n",
        "    train_every = 1\n",
        "\n",
        "    # The paths for saving the logs and learning curves\n",
        "    log_dir = './experiments/blackjack_dqn_result/'\n",
        "\n",
        "    # Set a global seed\n",
        "    set_global_seed(0)\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "\n",
        "        # Initialize a global step\n",
        "        global_step = tf.Variable(0, name='global_step', trainable=False)\n",
        "\n",
        "        # Set up the agents\n",
        "        agent = DQNAgent(sess,\n",
        "                         scope='dqn',\n",
        "                         action_num=env.action_num,\n",
        "                         replay_memory_init_size=memory_init_size,\n",
        "                         train_every=train_every,\n",
        "                         state_shape=env.state_shape,\n",
        "                         mlp_layers=[10,10])\n",
        "        env.set_agents([agent])\n",
        "        eval_env.set_agents([agent])\n",
        "\n",
        "        # Initialize global variables\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "\n",
        "        # Initialize a Logger to plot the learning curve\n",
        "        logger = Logger(log_dir)\n",
        "\n",
        "        for iteration in range(iteration_num):\n",
        "\n",
        "            # Generate data from the environment\n",
        "            trajectories, _ = env.run(is_training=True)\n",
        "\n",
        "            # Feed transitions into agent memory, and train the agent\n",
        "            for ts in trajectories[0]:\n",
        "                agent.feed(ts)\n",
        "\n",
        "            # Evaluate the performance. Play with random agents.\n",
        "            if iteration % evaluate_every == 0:\n",
        "                logger.log_performance(env.timestep, tournament(eval_env, evaluate_num)[0])\n",
        "        \n",
        "        # Save model\n",
        "        save_dir = 'models/blackjack_dqn'\n",
        "        if not os.path.exists(save_dir):\n",
        "            os.makedirs(save_dir)\n",
        "        saver = tf.train.Saver()\n",
        "        saver.save(sess, os.path.join(save_dir, 'model'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EofrekuMdgn",
        "colab_type": "code",
        "outputId": "64b0c10e-39e0-4784-ad7e-463f05628164",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/rlcard/utils/utils.py:332: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/rlcard/agents/dqn_agent.py:239: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/rlcard/agents/dqn_agent.py:256: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/rlcard/agents/dqn_agent.py:267: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/normalization.py:327: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1634: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/rlcard/agents/dqn_agent.py:280: The name tf.squared_difference is deprecated. Please use tf.math.squared_difference instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/rlcard/agents/dqn_agent.py:242: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/rlcard/agents/dqn_agent.py:242: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/rlcard/agents/dqn_agent.py:242: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/rlcard/agents/dqn_agent.py:244: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/rlcard/agents/dqn_agent.py:247: get_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.get_global_step\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  4\n",
            "  reward       |  -0.7399\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 100, rl-loss: 1.0956813097000122WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/rlcard/agents/dqn_agent.py:364: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n",
            "\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 541, rl-loss: 0.6026347279548645\n",
            "----------------------------------------\n",
            "  timestep     |  541\n",
            "  reward       |  -0.066\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 1099, rl-loss: 0.6058878898620605\n",
            "----------------------------------------\n",
            "  timestep     |  1099\n",
            "  reward       |  -0.0691\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 1100, rl-loss: 0.4600841701030731\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 1627, rl-loss: 0.5869379043579102\n",
            "----------------------------------------\n",
            "  timestep     |  1627\n",
            "  reward       |  -0.0753\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 2100, rl-loss: 0.6174850463867188\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 2189, rl-loss: 0.5101578235626221\n",
            "----------------------------------------\n",
            "  timestep     |  2189\n",
            "  reward       |  -0.0694\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 2726, rl-loss: 0.43811458349227905\n",
            "----------------------------------------\n",
            "  timestep     |  2726\n",
            "  reward       |  -0.0556\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 3100, rl-loss: 0.7509120106697083\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 3280, rl-loss: 0.5478805303573608\n",
            "----------------------------------------\n",
            "  timestep     |  3280\n",
            "  reward       |  -0.0853\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 3839, rl-loss: 0.5439354777336121\n",
            "----------------------------------------\n",
            "  timestep     |  3839\n",
            "  reward       |  -0.0782\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 4100, rl-loss: 0.5503383278846741\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 4397, rl-loss: 0.779647707939148\n",
            "----------------------------------------\n",
            "  timestep     |  4397\n",
            "  reward       |  -0.0781\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 4950, rl-loss: 0.5241678953170776\n",
            "----------------------------------------\n",
            "  timestep     |  4950\n",
            "  reward       |  -0.0689\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 5100, rl-loss: 0.5438821911811829\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 5506, rl-loss: 0.5144636631011963\n",
            "----------------------------------------\n",
            "  timestep     |  5506\n",
            "  reward       |  -0.0684\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 6071, rl-loss: 0.4934130012989044\n",
            "----------------------------------------\n",
            "  timestep     |  6071\n",
            "  reward       |  -0.0633\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 6100, rl-loss: 0.42859354615211487\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 6644, rl-loss: 0.6766248345375061\n",
            "----------------------------------------\n",
            "  timestep     |  6644\n",
            "  reward       |  -0.0664\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 7100, rl-loss: 0.6252628564834595\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 7221, rl-loss: 0.8283538818359375\n",
            "----------------------------------------\n",
            "  timestep     |  7221\n",
            "  reward       |  -0.0758\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 7774, rl-loss: 0.46493014693260193\n",
            "----------------------------------------\n",
            "  timestep     |  7774\n",
            "  reward       |  -0.065\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 8100, rl-loss: 0.6778500080108643\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 8348, rl-loss: 0.5381323099136353\n",
            "----------------------------------------\n",
            "  timestep     |  8348\n",
            "  reward       |  -0.0696\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 8897, rl-loss: 0.6368733644485474\n",
            "----------------------------------------\n",
            "  timestep     |  8897\n",
            "  reward       |  -0.0702\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 9100, rl-loss: 0.5569663643836975\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 9467, rl-loss: 0.6883324980735779\n",
            "----------------------------------------\n",
            "  timestep     |  9467\n",
            "  reward       |  -0.0489\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 10033, rl-loss: 0.5263139009475708\n",
            "----------------------------------------\n",
            "  timestep     |  10033\n",
            "  reward       |  -0.0703\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 10100, rl-loss: 0.6863312721252441\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 10636, rl-loss: 0.4928438365459442\n",
            "----------------------------------------\n",
            "  timestep     |  10636\n",
            "  reward       |  -0.0602\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 11100, rl-loss: 0.5286944508552551\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 11218, rl-loss: 0.5120810270309448\n",
            "----------------------------------------\n",
            "  timestep     |  11218\n",
            "  reward       |  -0.0742\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 11796, rl-loss: 0.4883652329444885\n",
            "----------------------------------------\n",
            "  timestep     |  11796\n",
            "  reward       |  -0.0656\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 12100, rl-loss: 0.5364532470703125\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 12413, rl-loss: 0.5073115229606628\n",
            "----------------------------------------\n",
            "  timestep     |  12413\n",
            "  reward       |  -0.0717\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 13001, rl-loss: 0.7639986276626587\n",
            "----------------------------------------\n",
            "  timestep     |  13001\n",
            "  reward       |  -0.0728\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 13100, rl-loss: 0.6749976873397827\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 13584, rl-loss: 0.394100159406662\n",
            "----------------------------------------\n",
            "  timestep     |  13584\n",
            "  reward       |  -0.0566\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 14100, rl-loss: 0.41783714294433594\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 14163, rl-loss: 0.6457048058509827\n",
            "----------------------------------------\n",
            "  timestep     |  14163\n",
            "  reward       |  -0.0508\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 14772, rl-loss: 0.6962716579437256\n",
            "----------------------------------------\n",
            "  timestep     |  14772\n",
            "  reward       |  -0.0652\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 15100, rl-loss: 0.5519866943359375\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 15370, rl-loss: 0.48996108770370483\n",
            "----------------------------------------\n",
            "  timestep     |  15370\n",
            "  reward       |  -0.0803\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 15971, rl-loss: 0.6750391721725464\n",
            "----------------------------------------\n",
            "  timestep     |  15971\n",
            "  reward       |  -0.071\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 16100, rl-loss: 0.49468088150024414\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 16581, rl-loss: 0.5764397382736206\n",
            "----------------------------------------\n",
            "  timestep     |  16581\n",
            "  reward       |  -0.0683\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 17100, rl-loss: 0.4728034734725952\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 17179, rl-loss: 0.4820243716239929\n",
            "----------------------------------------\n",
            "  timestep     |  17179\n",
            "  reward       |  -0.0646\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 17788, rl-loss: 0.40933650732040405\n",
            "----------------------------------------\n",
            "  timestep     |  17788\n",
            "  reward       |  -0.0823\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 18100, rl-loss: 0.540005624294281\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 18408, rl-loss: 0.5852022171020508\n",
            "----------------------------------------\n",
            "  timestep     |  18408\n",
            "  reward       |  -0.0628\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 19022, rl-loss: 0.7219164967536926\n",
            "----------------------------------------\n",
            "  timestep     |  19022\n",
            "  reward       |  -0.0915\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 19100, rl-loss: 0.6447409391403198\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 19628, rl-loss: 0.4258766770362854\n",
            "----------------------------------------\n",
            "  timestep     |  19628\n",
            "  reward       |  -0.0732\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 20100, rl-loss: 0.6021704077720642\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 20228, rl-loss: 0.5215957164764404\n",
            "----------------------------------------\n",
            "  timestep     |  20228\n",
            "  reward       |  -0.0711\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 20851, rl-loss: 0.4154014587402344\n",
            "----------------------------------------\n",
            "  timestep     |  20851\n",
            "  reward       |  -0.0629\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 21100, rl-loss: 0.5164810419082642\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 21465, rl-loss: 0.4558759033679962\n",
            "----------------------------------------\n",
            "  timestep     |  21465\n",
            "  reward       |  -0.0687\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 22082, rl-loss: 0.5519828200340271\n",
            "----------------------------------------\n",
            "  timestep     |  22082\n",
            "  reward       |  -0.0629\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 22100, rl-loss: 0.5951641798019409\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 22692, rl-loss: 0.4516165256500244\n",
            "----------------------------------------\n",
            "  timestep     |  22692\n",
            "  reward       |  -0.0678\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 23100, rl-loss: 0.723185122013092\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 23317, rl-loss: 0.7588373422622681\n",
            "----------------------------------------\n",
            "  timestep     |  23317\n",
            "  reward       |  -0.0497\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 23963, rl-loss: 0.4788249433040619\n",
            "----------------------------------------\n",
            "  timestep     |  23963\n",
            "  reward       |  -0.0737\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 24100, rl-loss: 0.5787351131439209\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 24575, rl-loss: 0.6130774021148682\n",
            "----------------------------------------\n",
            "  timestep     |  24575\n",
            "  reward       |  -0.068\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 25100, rl-loss: 0.5046749114990234\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 25184, rl-loss: 0.6263934373855591\n",
            "----------------------------------------\n",
            "  timestep     |  25184\n",
            "  reward       |  -0.0686\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 25840, rl-loss: 0.5344563722610474\n",
            "----------------------------------------\n",
            "  timestep     |  25840\n",
            "  reward       |  -0.0743\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 26100, rl-loss: 0.7072450518608093\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 26456, rl-loss: 0.480413019657135\n",
            "----------------------------------------\n",
            "  timestep     |  26456\n",
            "  reward       |  -0.0655\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 27092, rl-loss: 0.6483006477355957\n",
            "----------------------------------------\n",
            "  timestep     |  27092\n",
            "  reward       |  -0.0747\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 27100, rl-loss: 0.5688895583152771\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 27731, rl-loss: 0.5380850434303284\n",
            "----------------------------------------\n",
            "  timestep     |  27731\n",
            "  reward       |  -0.0533\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 28100, rl-loss: 0.48328897356987\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 28350, rl-loss: 0.6657267808914185\n",
            "----------------------------------------\n",
            "  timestep     |  28350\n",
            "  reward       |  -0.0903\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 28965, rl-loss: 0.5914641618728638\n",
            "----------------------------------------\n",
            "  timestep     |  28965\n",
            "  reward       |  -0.0677\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 29100, rl-loss: 0.5041288137435913\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 29584, rl-loss: 0.4957030117511749./experiments/blackjack_dqn_result/performance.csv\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHRo7qjueCak",
        "colab_type": "text"
      },
      "source": [
        "Good job! Now you have your trianed Blackjack game with multiple processes."
      ]
    }
  ]
}